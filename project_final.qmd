---
title: "PROJECT"
subtitle: "BLAL BLA BLA"
author: anonymous
format:
  html:
    toc: true
    code-tools: true
    code-line-numbers: true
    msainfont: Georgia, serif
    page-layout: article
  pdf:  
    number-sections: true
    code-annotations: none
editor: source
---

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=false}
 
## Setup

Install packages:

```{r}
#| label: Setup
remotes::install_github("higgi13425/medicaldata")
library(medicaldata)
library(dplyr)
library(brms)
library(corrplot)
library(rstanarm)
library(ggplot2)
library(loo)
library(rstanarm)
library(caret)
library(splines)
library(MASS)
# library(RColorBrewer)
library(gridExtra)
library(grid)
```

Import the data:

```{r}
#| label: Importing data
cath <- medicaldata::cath
```

:::
::::


# Introduction 

- Background
- Problem formulation/scope
- Main modeling idea
- Some picture of the data?

# Data Desctription

- Dataset:

```{r}
#| label: Data description
#| echo: false
#| message: false
#| warning: false

cath_dims <- dim(cath)
cat("Number of variables: ", cath_dims[2], "\n")
cat("Number of observations: ", cath_dims[1])
knitr::kable(head(cath))
```

Histograms and barplot of the explanatory variables:

```{r}
#| label: Univariate analysis part 1
#| echo: false
#| fig-cap: TBD 1

variable_names = names(cath)

colors = c("#F4CAE4", "#E6F5C9")
par(mfrow = c(1,4))
barplot(table(cath[,1]), main = variable_names[1], 
  xlab = variable_names[1], col = colors[1])
for(i in 2:4){
  hist(cath[[i]], main = variable_names[i], 
    xlab = variable_names[i], col = colors[1])
}

```

Barplots of the responsive variables:

```{r}
#| label: Univariate analysis part 2
#| echo: false
#| fig-cap: TBD 2

par(mfrow = c(1,2))
for(i in 5:6){
  barplot(table(cath[,i]), main = variable_names[i], 
    xlab = variable_names[i], col = colors[2])
}
```

Explanation on data preprocessing (removing NA rows, scaling variables, removing tvdlm etc.)

```{r}
#| label: Data preprocessing
#| echo: false
#| warning: false
#| message: false

# Remove tvdlm column
# Remove rows with at least one NA value
cath <- cath %>%
  na.omit() %>%
  dplyr::select(-tvdlm)

# Scale the variables
for (i in 1:(ncol(cath)-1)){
    cath[i] <- scale(cath[i])
}

# Print out dimensions after preprocessing
cath_dims <- dim(cath)
expl_var <- cath_dims[2]-1
n_obs <- cath_dims[1]
cat("After preprocessing we have:\n")
cat("Number of predictive variables: ", expl_var, "\n")
cat("Number of observations: ", n_obs)
```

- Histograms
- Correlations
- Has it been used in previous studies?

Bivariate correlations:

```{r}
#| label: Bivariate analysis
#| echo: false
#| fig-cap: TBD 3
p <- ncol(cath)
par(mfrow = c(1,1))
corrplot(cor(cath[, c(p,1:(p-1))]), type = "full", method = "number")
```

Seems that sex and age have the most significant bivariate correlation to the responsive variable sigdz.

# Mathematical Model

- Bayesian logistic regression
- Likelihood
- Priors
- Posteriors

Motivation: 
  - Logistic regression gives the estimate of event probability. 
  - Bayesian logistic regression gives also estimate on the *uncertainty* of the predicted probability OR the model itself.

Probability of success: $P(y=1) = Sigmoid(w^Tx+b)$
  - Sigmoid maps the output to interval [0, 1]

Bayesian approach: 
  - Instead of learning the weights, we learn the *distribution of the weights*: $P(y=1|w) = Sigmoid(w^Tx+b)$

For M = 'r expl_var' variables and N = 'r n_obs'. Then:

  For each variable ($j = 1,...,M$), the **prior** is $w_j \sim prior(w_j)$
    $\rightarrow$ We define the prior to the **weights** of each variable
  
  For each datapoint $x_i$ and outcome $y_i$ ($i=1,...,N$): 
    $\epsilon_i = Sigmoid(w^Tx_i+b)$
    $y_i \sim Bernoulli(\epsilon_i)$


# Model Definitions and Implementation

- Tie mathematical model to our implementation
- Ndraws, warmup, etc.

## Linear model

```{r}
#| label: Linear model definition
#| warning: false
#| message: false

# Make response variable a factor
cath$sigdz <- as.factor(cath$sigdz)

x <- model.matrix(sigdz ~ . - 1, data=cath)
y <- cath$sigdz

# Formula
formula_linear <- formula(sigdz ~ sex + age + cad_dur + choleste)

# Prior
prior_linear <- student_t(df = 3, location = 0, scale = 2.5)

# The model
# model_linear <- stan_glm(formula_linear, data = cath,
#                  family = binomial(link = "logit"), 
#                  prior = prior_linear, prior_intercept = prior_linear, QR=TRUE,
#                  refresh=0)

# saveRDS(model_linear, file = "./additional_files/model_linear.rds")
model_linear <- readRDS("./additional_files/model_linear.rds")                
```

## Nonlinear model

```{r}
#| label: Nonlinear model definition
#| warning: false
#| message: false

# Formula
formula_nonlinear <- formula(sigdz ~ sex + s(age) + s(cad_dur) + s(choleste))

# Model definition
# model_nonlinear <- stan_gamm4(
#   formula_nonlinear, data = cath,
#   family = binomial(link = "logit",
#   refresh = 0)
# )

# saveRDS(model_nonlinear, file = "./additional_files/model_nonlinear.rds")
model_nonlinear <- readRDS("./additional_files/model_nonlinear.rds")
```

# Model Evaluation
- Rhat, ESS, HMC divergences, pp_check(), loo_compare(), classification accuracy
- Prior sensitivity analysis

## Convergence diagnostics & posterior predictive checks

```{r}
#| label: Linear model - Posterior predictive check
summary(model_linear)
pp_check(model_linear)
```

Nonlinear model:

```{r}
#| label: Nonlinear model - Posterior predictive check
summary(model_nonlinear)
pp_check(model_nonlinear)
```

Both:

```{r}
#| label: Plotting the uncertainty of intercepts
pplot_linear<-plot(model_linear, "areas", prob = 0.95, prob_outer = 1)
pplot_linear + geom_vline(xintercept = 0)

pplot_nonlinear<-plot(model_nonlinear, "areas", prob = 0.95, prob_outer = 1)
pplot_nonlinear + geom_vline(xintercept = 0)
```

## Model comparison using LOO-CV

```{r}
#| label: Computing LOOs
# LOO of our models
loo_linear <- loo(model_linear, save_psis = TRUE)
loo_nonlinear <- loo(model_nonlinear, save_psis = TRUE)
print(loo_linear)
print(loo_nonlinear)

# Baseline model of the linear model
model_baseline <- update(model_linear, formula = sigdz ~ 1, QR = FALSE)

# LOO of the baseline model
loo_baseline <- loo(model_baseline, save_psis = TRUE)
```

Comparing the LOOs:

```{r}
#| label: Comparing LOOs
loo_compare(loo_linear, loo_nonlinear, loo_baseline)
```

The nonlinear model seems to be the best!

## Posterior predictive performance: Classification accuracy

### Linear model:

```{r}
#| label: Linear model - Predictive probs
## Predicted probabilities
linpred_linear <- posterior_linpred(model_linear)
preds_linear <- posterior_epred(model_linear)
pred_linear <- colMeans(preds_linear)
pr_linear <- as.integer(pred_linear >= 0.5)
   
## posterior classification accuracy
round(mean(xor(pr_linear,as.integer(y==0))),2)

# posterior balanced classification accuracy
round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Linear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_linear=E_loo(preds_linear, loo_linear$psis_object, type="mean", log_ratios = -log_lik(model_linear))$value

# LOO classification accuracy
round(mean(xor(ploo_linear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Nonlinear model

```{r}
#| label: Nonlinear model - Predictive probs
## Predicted probabilities
linpred_nonlinear <- posterior_linpred(model_nonlinear)
preds_nonlinear <- posterior_epred(model_nonlinear)
pred_nonlinear <- colMeans(preds_nonlinear)
pr_nonlinear <- as.integer(pred_nonlinear >= 0.5)
   
## posterior classification accuracy
round(mean(xor(pr_nonlinear,as.integer(y==0))),2)

# posterior balanced classification accuracy
round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Nonlinear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_nonlinear=E_loo(preds_nonlinear, loo_nonlinear$psis_object, type="mean", log_ratios = -log_lik(model_nonlinear))$value

# LOO classification accuracy
round(mean(xor(ploo_nonlinear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_nonlinear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Calibration curves 

```{r}
#| label: Calibration plots
#| fig-cap: TBD X
cplot_linear <- ggplot(data = data.frame(pred=pred_linear,loopred=ploo_linear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) +
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') +
  labs(x = "Predicted (LOO)",y = "Observed",title = "Linear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) +
  scale_y_continuous(breaks=seq(0,1,by=0.1)) +
  xlim(c(0,1)) +
  theme_minimal()

cplot_nonlinear <- ggplot(data = data.frame(loopred=ploo_nonlinear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) + 
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') + 
  labs(x = "Predicted (LOO)",y = "Observed",title = "Nonlinear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) + 
  scale_y_continuous(breaks=seq(0,1,by=0.1)) + 
  xlim(c(0,1)) +
  theme_minimal() 

grid.arrange(cplot_linear, cplot_nonlinear, ncol=2)
```


# Discussion
- Problems, potential improvements?
- Conclusion of analysis

# Lessons Learned


# References

