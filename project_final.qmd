---
title: "PROJECT"
subtitle: "BLAL BLA BLA"
author: anonymous
format:
  html:
    toc: true
    code-tools: true
    code-line-numbers: true
    msainfont: Georgia, serif
    page-layout: article
  pdf:  
    number-sections: true
    code-annotations: none
editor: source
---

::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse="false"}
## Setup

Install packages:

```{r}
#| label: Setup
remotes::install_github("higgi13425/medicaldata")
library(medicaldata)
library(dplyr)
library(brms)
library(corrplot)
library(rstanarm)
library(ggplot2)
library(loo)
library(rstanarm)
library(caret)
library(splines)
library(MASS)
# library(RColorBrewer)
library(gridExtra)
library(grid)
seed = 1234
```

Import the data:

```{r}
#| label: Importing-data
cath <- medicaldata::cath
```
:::
:::

# Introduction

-   Background
-   Problem formulation/scope
-   Main modeling idea
-   Some picture of the data?

# Data Desctription

The "cath" dataset used in this report is obtained from Duke University Cardiovascular Disease Databank. It encapsulates a collection of 6 variables (@Data-table) that are closely related to cardiovascular health.

```{r}
#| label: Data-table
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: TBD
knitr::kable(head(cath))
```

The dataset consists of four explanatory variables (*sex*, *age*, *cad_dur*, *choleste*) and two response variables (*sigdz*, *tvdlm*) that provide an overview on patient demographics, clinical indicators, and critical outcomes related to coronary artery disease:

-   **Sex** (*sex*): Categorized as 0 for male and 1 for female, this variable represents the gender distribution within our dataset.

-   **Age** (*age*): Representing the age of patients in years, this variable serves as a demographic feature.

-   **Chest Pain Duration** (*cad_dur*): The duration of chest pain symptoms in days.

-   **Serum Cholesterol Level** (*choleste*): Measured in milligrams per deciliter, serum cholesterol levels are indicative of lipid metabolism and play a crucial role in cardiovascular health.

-   **Significant Coronary Disease** (*sigdz*): A binary variable that captures the presence (1) or absence (0) of at least 75% blockage in one of the major coronary arteries.

-   **Three Vessel Disease or Left Main Disease** (*tvdlm*): Denoting the presence (1) or absence (0) of blockage in either all three coronary vessels or in the left main coronary artery.

The univariate distributions of these variables are described in @Univariate-analysis.

```{r}
#| label: Univariate-analysis
#| echo: false
#| fig-cap: TBD 1

par(mfrow = c(2,3), oma = c(4,4,0.1,0.1)) # TBD

variable_names = names(cath)
colors = c("#F4CAE4", "#E6F5C9")

# Explanatory variables
barplot(table(cath[,1]), main = variable_names[1], 
  xlab = variable_names[1], col = colors[1])
for(i in 2:4){
  hist(cath[[i]], main = variable_names[i], 
    xlab = variable_names[i], col = colors[1])
}

# Response variables
for(i in 5:6){
  barplot(table(cath[,i]), main = variable_names[i], 
    xlab = variable_names[i], col = colors[2])
}
```

While constructing the Bayesian models to predict the probaility of significant coronary disease, the report strives to utilize the correlation between the explanatory variables (*sex*, *age*, *cad_dur*, *choleste*) and the desired response variable (*sigdz*). The *tvdlm* variable is not relevant in this report as the main focus is to predict the probability of significant coronary disease, independent of the type of the blockade.

```{r}
#| label: Data-preprocessing
#| echo: false
#| warning: false
#| message: false

# Remove tvdlm column
# Remove rows with at least one NA value
cath <- cath %>%
  na.omit() %>%
  dplyr::select(-tvdlm)

# Scale the variables
for (i in 1:(ncol(cath)-1)){
    cath[i] <- scale(cath[i])
}

# Dimensions after preprocessing
cath_dims <- dim(cath)
n_obs <- cath_dims[1]
```

Before the analysis, the data is preprocessed by removing *tvdlm* column and all rows that contain missing values, as well as by scaling the continuous variables to zero mean and unit variance. After this, we are left with $n =$ `r n_obs` observations. The pairwise correlations of variables are described in @Bivariate-analysis. We can see that sex and age have the most significant bivariate correlation to the responsive variable sigdz.

```{r}
#| label: Bivariate-analysis
#| echo: false
#| fig-cap: TBD 3

par(mfrow=c(1,1))
p <- ncol(cath)

corrplot(cor(cath[, c(p,1:(p-1))]), type = "full", method = "number")
```

# Mathematical Model

To-be-done still: - Check likelihood notation for non-linear model - Prior justification - Check prior notations - Include priors with own values ? - Posteriors ?

## The generalized linear model and priors

For the binary response variable $y$, the likelihood for the binomial GLM can be written as a conditionally binomial PMF.

$$\binom{n}{y} \pi (1 - \pi)^{n-y},$$

where n is the total amount of trials, $\pi = g^{-1}(\eta)$ is the probability of success (patient presenting with significant coronary disease) and $\eta = \alpha + x^{T} \beta$ is a linear predictor. For a sample of size N, the likelihood is the product of the N individual likelihoods.

The link function $g$ maps the probability $\pi$ between the unit interval and the set of real numbers $\mathbb{R}$, and when applying the inverse link function to the linear predictor $\eta H$ the output will be a valid probability between 0 and 1.

This project utilizes the logit link function for the GLM, which makes it a logistic regression model. The likelihood expressed with the logit link function $g(x) = \text{ln}(\frac{x}{1-x})$ for a single observation can be written as:

$$\binom{n}{y} (\text{logit}^{-1} (\eta))^y (1 - \text{logit}^{-1})^{n-y}$$

Priors are set for the intercept and vector of regression coefficients $\alpha$ and $\beta$, from the linear predictor $\eta = \alpha + x^{T} \beta$,

$$
\begin{aligned}
\alpha &\sim t_v(\mu, \sigma^2) \\
\beta_k &\sim t_v(\mu, \sigma^2) \\
\end{aligned}
$$ 

where $v$ is the degrees of freedom, $\mu$ is the location and $\sigma$ is the scale.

The intercept and the regression coefficients are believed to be as likely positive as they would be negative, but likely relatively close to zero. This can be represented with normal distribution with a mean of zero and a small standard deviation. For example, $\mathcal{N}(0, 1)$. The priors can also be represented with the Students t-distribution if there is less priori confidence that the parameters will be close to zero. The Students t-distribution includes a larger standard deviation and has heavier tails than the normal distribution and would therefore be suitable for this purpose.

## The additive non-linear model

The additive non-linear model on the other hand combines multiple functions in a way that isn't strictly linear. This allows for a more flexible relationship between the explanatory and response variable $y$. In this report, the non-linear model uses the same link function, and the logistic regression model can be written as:

$$
\text{logit} (\pi_i) = \beta_0 + \beta_1 f_1(x_1) + \beta_2 f_2(x_2) ... \beta_n f_n(x_n),
$$

where $f_i$ are non-linear functions that transform the explanatory features individually. In this report, all functions are smooth functions except $f_{age}$, which would only be the variable itself. The smooth functions utilize penalized splines, allowing the model to create a curved relationship between the features. The shape of the smooth functions is estimated from the data and the penalty helps avoid overfitting.nThe smooth function works by minimizing the sum of the model fit with the smoothness / penalty (here, thin plate regression splines (default)).

The likelihood would then become:

$$
\prod_{i = 1}^{N}\text{Bernoulli}(y|\text{logit}^{-1}(\pi_i),  \sigma)
$$

# Model Definitions and Implementation

TO DO:
-   Explain in more detail what stan_glm and stan_gamm4 do
-   check if line 218 is necessary (x <- model.matrix(sigdz ~ . - 1, data=cath))
    IF NOT: --> remove commented code

The linear and non-linear stan models are implemented with the rstanarm package functions as described below. Both models were implemented with identical number of chains, draws and warm-up. The default values (chains = 4, draws = 4000, and warmup = 2000) were used in both cases.

## Linear model

The linear model was implemented with the help of the `stan_glm` function from the rstanarm package and defined to a logistic regression model with the help of the model parameter `family = binomial(link = 'logit')`. 

The linear relationship between the response variable `sigdz` and the explanatory variables are defined with the help of the `formula` function and the prior for the regression coefficients and intercept with the help of the `student_t` function. 



```{r}
#| label: Linear-model-definition
#| warning: false
#| message: false

# Make response variable a factor
cath$sigdz <- as.factor(cath$sigdz)

#x <- model.matrix(sigdz ~ . - 1, data=cath)
y <- cath$sigdz

# Formula
formula_linear <- formula(sigdz ~ sex + age + cad_dur + choleste)

# Prior
prior_linear <- student_t(df = 3, location = 0, scale = 2.5)

# The model
model_linear <- stan_glm(formula_linear, data = cath,
                 family = binomial(link = "logit"), 
                 prior = prior_linear, prior_intercept = prior_linear,
                  QR=TRUE, refresh=0)

# saveRDS(model_linear, file = "./additional_files/model_linear.rds")
# model_linear <- readRDS("./additional_files/model_linear.rds")                
```

## Nonlinear model

The non-linear model is simliarly implemented `stan_gamm4` function from the rstanarm package and defined to a logistic regression model with the help of the model parameter `family = binomial(link = 'logit')`. 

The non-linear relationship between the response variable `sigdz` and the explanatory variables is again defined with the help of the `formula` function. This time, passing the smooth function `s()` for all continuous explanatory variables, to allow for more complexity in the model, while simultaneously penalizing over-fitting of the model with the the thin plate regression splines smoothness.  

The same priors are used as for the linear model for both the intercept and the regression coefficients.

```{r}
#| label: Nonlinear model definition
#| warning: false
#| message: false

# Formula
formula_nonlinear <- formula(sigdz ~ sex + s(age) + s(cad_dur) + s(choleste))

# Model definition
model_nonlinear <- stan_gamm4(
  formula_nonlinear, data = cath,
  family = binomial(link = "logit"),
  prior = prior_linear, prior_intercept = prior_linear,
  iter = 4000, warmup = 2000, refresh = 0
)

# saveRDS(model_nonlinear, file = "./additional_files/model_nonlinear.rds")
# model_nonlinear <- readRDS("./additional_files/model_nonlinear.rds")
```


# Model Evaluation

After fitting the models, multiple evaluation metrics such as $\hat{R}$, effective sample size (ESS) and number of divergent transitions are used to assess the convergence of MCMC chains separately for each model. Additionally, we perform posterior predictive checks, assess the model performances as well as compare the models utilizing leave-one-out cross validation (LOO-CV). 


-   Rhat, ESS, HMC divergences, pp_check(), loo_compare(), classification accuracy
-   Prior sensitivity analysis

## Convergence diagnostics & posterior predictive checks

```{r}
#| label: Linear model - Posterior predictive check
summary(model_linear)
model_linear$stan_summary[, "Rhat"] %>% round(3)

brms::pp_check(model_linear)
```

Nonlinear model:

```{r}
#| label: Nonlinear model - Posterior predictive check
summary(model_nonlinear)
pp_check(model_nonlinear)
```

Both:

```{r}
#| label: Plotting the uncertainty of intercepts
pplot_linear<-plot(model_linear, "areas", prob = 0.95, prob_outer = 1)
pplot_linear + geom_vline(xintercept = 0)

# pplot_nonlinear<-plot(model_nonlinear, "areas", prob = 0.95, prob_outer = 1)
# pplot_nonlinear + geom_vline(xintercept = 0)
```

## Model comparison using LOO-CV

```{r}
#| label: Computing LOOs
# LOO of our models
loo_linear <- loo(model_linear, save_psis = TRUE)
loo_nonlinear <- loo(model_nonlinear, save_psis = TRUE)
print(loo_linear)
print(loo_nonlinear)

# Baseline model of the linear model
model_baseline <- update(model_linear, formula = sigdz ~ 1, QR = FALSE)

# LOO of the baseline model
loo_baseline <- loo(model_baseline, save_psis = TRUE)
```

Comparing the LOOs:

```{r}
#| label: Comparing LOOs
loo_compare(loo_linear, loo_nonlinear, loo_baseline)
```

The nonlinear model seems to be the best!

## Posterior predictive performance: Classification accuracy

### Linear model:

```{r}
#| label: Linear model - Predictive probs
## Predicted probabilities
linpred_linear <- posterior_linpred(model_linear)
preds_linear <- posterior_epred(model_linear)
pred_linear <- colMeans(preds_linear)
pr_linear <- as.integer(pred_linear >= 0.5)
   
## posterior classification accuracy
round(mean(xor(pr_linear,as.integer(y==0))),2)

# posterior balanced classification accuracy
round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Linear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_linear=E_loo(preds_linear, loo_linear$psis_object, type="mean", log_ratios = -log_lik(model_linear))$value

# LOO classification accuracy
round(mean(xor(ploo_linear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Nonlinear model

```{r}
#| label: Nonlinear model - Predictive probs
## Predicted probabilities
linpred_nonlinear <- posterior_linpred(model_nonlinear)
preds_nonlinear <- posterior_epred(model_nonlinear)
pred_nonlinear <- colMeans(preds_nonlinear)
pr_nonlinear <- as.integer(pred_nonlinear >= 0.5)
   
## posterior classification accuracy
round(mean(xor(pr_nonlinear,as.integer(y==0))),2)

# posterior balanced classification accuracy
round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Nonlinear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_nonlinear=E_loo(preds_nonlinear, loo_nonlinear$psis_object, type="mean", log_ratios = -log_lik(model_nonlinear))$value

# LOO classification accuracy
round(mean(xor(ploo_nonlinear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_nonlinear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Calibration curves

```{r}
#| label: Calibration plots
#| fig-cap: TBD X
cplot_linear <- ggplot(data = data.frame(pred=pred_linear,loopred=ploo_linear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) +
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') +
  labs(x = "Predicted (LOO)",y = "Observed",title = "Linear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) +
  scale_y_continuous(breaks=seq(0,1,by=0.1)) +
  xlim(c(0,1)) +
  theme_minimal()

cplot_nonlinear <- ggplot(data = data.frame(loopred=ploo_nonlinear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) + 
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') + 
  labs(x = "Predicted (LOO)",y = "Observed",title = "Nonlinear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) + 
  scale_y_continuous(breaks=seq(0,1,by=0.1)) + 
  xlim(c(0,1)) +
  theme_minimal() 

grid.arrange(cplot_linear, cplot_nonlinear, ncol=2)
```

# Discussion

-   Problems, potential improvements?
-   Conclusion of analysis

# Lessons Learned

# References
