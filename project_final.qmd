---
title: "PROJECT"
subtitle: "BLAL BLA BLA"
author: anonymous
format:
  html:
    toc: true
    code-tools: true
    code-line-numbers: true
    msainfont: Georgia, serif
    page-layout: article
  pdf:  
    number-sections: true
    code-annotations: none
editor: source
---

::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse="false"}
## Setup

Install packages:

```{r}
#| label: Setup
remotes::install_github("higgi13425/medicaldata")
library(medicaldata)
library(dplyr)
library(brms)
library(corrplot)
library(rstanarm)
library(ggplot2)
library(loo)
library(rstanarm)
library(caret)
library(splines)
library(MASS)
# library(RColorBrewer)
library(gridExtra)
library(grid)
seed = 1234
```

Import the data:

```{r}
#| label: Importing-data
cath <- medicaldata::cath
```
:::
:::

# Introduction

-   Background
-   Problem formulation/scope
-   Main modeling idea
-   Some picture of the data?

# Data Desctription

The "cath" dataset used in this report is obtained from Duke University Cardiovascular Disease Databank. It encapsulates a collection of 6 variables (@Data-table) that are closely related to cardiovascular health.

```{r}
#| label: Data-table
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: TBD
knitr::kable(head(cath))
```

The dataset consists of four explanatory variables (*sex*, *age*, *cad_dur*, *choleste*) and two response variables (*sigdz*, *tvdlm*) that provide an overview on patient demographics, clinical indicators, and critical outcomes related to coronary artery disease:

-   **Sex** (*sex*): Categorized as 0 for male and 1 for female, this variable represents the gender distribution within our dataset.

-   **Age** (*age*): Representing the age of patients in years, this variable serves as a demographic feature.

-   **Chest Pain Duration** (*cad_dur*): The duration of chest pain symptoms in days.

-   **Serum Cholesterol Level** (*choleste*): Measured in milligrams per deciliter, serum cholesterol levels are indicative of lipid metabolism and play a crucial role in cardiovascular health.

-   **Significant Coronary Disease** (*sigdz*): A binary variable that captures the presence (1) or absence (0) of at least 75% blockage in one of the major coronary arteries.

-   **Three Vessel Disease or Left Main Disease** (*tvdlm*): Denoting the presence (1) or absence (0) of blockage in either all three coronary vessels or in the left main coronary artery.

The univariate distributions of these variables are visualized in @Univariate-analysis.

```{r}
#| label: Univariate-analysis
#| echo: false
#| fig-cap: TBD 1

par(mfrow = c(2,3), oma = c(2,2,0.1,0.1)) # TBD

variable_names = names(cath)
colors = c("#F4CAE4", "#E6F5C9")

# Explanatory variables
barplot(table(cath[,1]), main = variable_names[1], 
  xlab = variable_names[1], col = colors[1])
for(i in 2:4){
  hist(cath[[i]], main = variable_names[i], 
    xlab = variable_names[i], col = colors[1])
}

# Response variables
for(i in 5:6){
  barplot(table(cath[,i]), main = variable_names[i], 
    xlab = variable_names[i], col = colors[2])
}
```

While constructing the Bayesian models to predict the probaility of significant coronary disease, the report strives to utilize the correlation between the explanatory variables (*sex*, *age*, *cad_dur*, *choleste*) and the desired response variable (*sigdz*). The *tvdlm* variable is not relevant in this report as the main focus is to predict the probability of significant coronary disease, independent of the type of the blockade.

```{r}
#| label: Data-preprocessing
#| echo: false
#| warning: false
#| message: false

# Remove tvdlm column
# Remove rows with at least one NA value
cath <- cath %>%
  na.omit() %>%
  dplyr::select(-tvdlm)

# Scale the variables
for (i in 1:(ncol(cath)-1)){
    cath[i] <- scale(cath[i])
}

# Dimensions after preprocessing
cath_dims <- dim(cath)
n_obs <- cath_dims[1]
```

Before the analysis, the data is preprocessed by removing *tvdlm* column and all rows that contain missing values, as well as by scaling the continuous variables to zero mean and unit variance. After this, we are left with $n =$ `r n_obs` observations. The pairwise correlations of variables are visualized in @Bivariate-analysis. We can see that variables *sex* and *age* have the most significant bivariate correlation to the responsive variable *sigdz*.

```{r}
#| label: Bivariate-analysis
#| echo: false
#| fig-cap: TBD 3

par(mfrow=c(1,1))
p <- ncol(cath)

corrplot(cor(cath[, c(p,1:(p-1))]), type = "full", method = "number")
```

# Mathematical Model

In this analysis, we will construct two models for inferring the binary response variable, *sigdz* based on input explanatory variables*.* The first model is a generalized linear model (GLM), namely Bayesian logistic regression. The other model is a generalized additive mixed model (GAMM), which implements Bayesian logistic regression with nonlinear transformations on the input variables. These models will be referred as linear and nonlinear model, respectively.

(((To-be-done still: - Check likelihood notation for nonlinear model - Prior justification - Check prior notations - Include priors with own values ? - **Posteriors** ?)))

## The generalized linear model and priors

**Elissan ehdottamat muokkaukset Linnean kirjoitukseen:**

Let $y$ be the number of times the variable *sigdz* is realized to be 1 for one individual in the dataset, and let $x$ be the explanatory variables for this outcome. Then, this number of successes for one individual follows a Binomial distribution

$$
y \sim \binom{n}{y}\theta^y(1-\theta)^{n-y},
$$

where n is the number of observations for that specific individual and $\theta = g^{-1}(\eta)$ $(\eta = \alpha + x^{T} \beta)$ is the probability of success (patient presenting with significant coronary disease). The inverse link function $g^{-1}$ maps the output of the linear predictor $\eta$ to a probability interval between 0 and 1.

For the binomial GLM, this project utilizes logit $g(x) = \ln(\frac{x}{1-x})$ as a link function, which makes it a logistic regression model. As each individual occurs only once in the data, $y$ can be directly presented as the binary response variable. Therefore, the likelihood of the response variable of one individual is reduced to Bernoulli distribution

$$
y \sim \text{ logit}^{-1}(\eta)^y(1-\text{ logit}^{-1}(\eta))^{1-y}.
$$

The complete data likelihood is then a product of $n =$ `r n_obs` likelihoods, with unshared probability of success.

As in Bayesian logistic regression the scope is to infer the distribution of the regression weights, namely the intercept $\alpha$ and coefficients $\beta = [\beta_1, \beta_2, \beta_3, \beta_4]^T$, we define the prior to be Student's $t$-distribution

$$
\begin{aligned} 
\alpha &\sim t_v(\mu, \sigma) \\ 
\beta_k &\sim t_v(\mu, \sigma), \ k=1,..,4 \\ 
\end{aligned} 
$$

where $v$ is the degrees of freedom, $\mu$ is the location and $\sigma$ is the scale.

The selection of prior distribution was done based on the nature of the data. Due to correlations, there is reason to believe that the parameters are not very close to zero, but most are still rather small than large. Therefore, as Student's $t$-distribution has heavy tails and larger scale compared to, for example, Gaussian distribution, $t$-distribution is a suitable choice of prior for this purpose. The parameters of the prior were defined to be

$$
v = 3, \quad \mu = 0, \quad \sigma = 2.5,
$$

as the coefficients can be positive or negative.

Finally, the joint posterior distribution that is simulated using Hamiltonian Monte Carlo (HMC) is proportional to the product of likelihood and prior distributions:

$$
p(\alpha, \beta | \bf{x}, \bf{y} ) \propto t_v(\alpha | \mu, \sigma) \times \prod^4_{k=1} t_v(\beta_k | \mu, \sigma) \times \prod^n_{i=1} \text{ logit}^{-1}(\eta_i)^{y_i}(1-\text{ logit}^{-1}(\eta_i))^{1-y_i}
$$

------------------------------------------------------------------------

**Linnean kirjoitus:**

For the binary response variable $y$, the likelihood for the binomial generalized linear model (GLM) can be written as a conditionally binomial PMF.

$$
\binom{n}{y} \pi (1 - \pi)^{n-y},
$$

where n is the total amount of trials, $\pi = g^{-1}(\eta)$ is the probability of success (patient presenting with significant coronary disease) and $\eta = \alpha + x^{T} \beta$ is a linear predictor. For a sample of size N, the likelihood is the product of the N individual likelihoods.

The link function $g$ maps the probability $\pi$ between the unit interval and the set of real numbers $\mathbb{R}$, and when applying the inverse link function to the linear predictor $\eta H$ the output will be a valid probability between 0 and 1.

This project utilizes the logit link function for the GLM, which makes it a logistic regression model. The likelihood expressed with the logit link function $g(x) = \text{ln}(\frac{x}{1-x})$ for a single observation can be written as:

$$\binom{n}{y} (\text{logit}^{-1} (\eta))^y (1 - \text{logit}^{-1}(\eta))^{n-y}$$

Priors are set for the intercept and vector of regression coefficients $\alpha$ and $\beta$, from the linear predictor $\eta = \alpha + x^{T} \beta$,

$$
\begin{aligned}
\alpha &\sim t_v(\mu, \sigma^2) \\
\beta_k &\sim t_v(\mu, \sigma^2) \\
\end{aligned}
$$

where $v$ is the degrees of freedom, $\mu$ is the location and $\sigma$ is the scale.

The intercept and the regression coefficients are believed to be as likely positive as they would be negative, but likely relatively close to zero. This can be represented with normal distribution with a mean of zero and a small standard deviation. For example, $\mathcal{N}(0, 1)$. The priors can also be represented with the Students t-distribution if there is less priori confidence that the parameters will be close to zero. The Students t-distribution includes a larger standard deviation and has heavier tails than the normal distribution and would therefore be suitable for this purpose.

## The generalized additive nonlinear model

The additive nonlinear model on the other hand combines multiple functions in a way that is not strictly linear. This allows for a more flexible relationship between the explanatory variables and response variable $y$. In this report, the nonlinear model uses the same link function as the linear model, and so the nonlinear predictor for the logistic regression model can be written as:

$$
\text{logit} (\theta) = \eta = \alpha + \sum^4_{k=1} \beta_k f_k(x_k),
$$

where $f_k$ are nonlinear functions that transform the explanatory features individually. In this report, all functions of the continuous variables are smoothing functions. The $f_{age}$ is simply the variable itself, due to *sex* being binary variable. The smoothing functions utilize penalized splines, allowing the model to create a curved relationship between the features. The shape of the smoothing functions is estimated from the data and the penalty helps avoid overfitting. The smoothing function works by minimizing the sum of the model fit with the smoothness / penalty (here, thin plate regression splines (default)).

The likelihood of observation is the same as with the linear model, with the difference that a nonlinear transformation is applied to all input explanatory variables $\bf{x}$. Additionally, we utilize the same prior in both models to make them as comparable as possible. The posterior is again similar as with the linear model, but with the difference of nonlinear transformations on the input data.

((t-distribution prior leads to 1 divergent transition with the nonlinear model -\> With default prior we got 0 divergent transitions -\> Otetaanko sittenkin eri priorit?))

# Model Definitions and Implementation

(((TO DO: - Explain in more detail what stan_glm and stan_gamm4 do - check if line 218 is necessary (x \<- model.matrix(sigdz \~ . - 1, data=cath)) IF NOT: --\> remove commented code)))

The linear and nonlinear models are implemented as Stan code with the rstanarm package as described below. Both models were implemented with identical number of chains, draws and warm-up. The default values (chains = 4, draws = 4000, and warmup = 2000) were used in both cases.

## Linear model

The linear model was implemented with the help of the `stan_glm` function from the rstanarm package and defined to be a logistic regression model with the help of the model parameter `family = binomial(link = 'logit')`.

The linear relationship between the response variable `sigdz` and the explanatory variables are defined with the help of the `formula` function and the prior for the regression coefficients and intercept with the help of the `student_t` function.

```{r}
#| label: Linear-model-definition
#| warning: false
#| message: false

# Make response variable a factor
cath$sigdz <- as.factor(cath$sigdz)

#x <- model.matrix(sigdz ~ . - 1, data=cath)
y <- cath$sigdz

# Formula
formula_linear <- formula(sigdz ~ sex + age + cad_dur + choleste)

# Prior
prior_linear <- student_t(df = 3, location = 0, scale = 2.5)

# The model
model_linear <- stan_glm(formula_linear, data = cath,
                 family = binomial(link = "logit"), 
                 prior = prior_linear, prior_intercept = prior_linear,
                QR=TRUE, refresh=0)

# saveRDS(model_linear, file = "./additional_files/model_linear.rds")
# model_linear <- readRDS("./additional_files/model_linear.rds")                
```

## Nonlinear model

The nonlinear model is similarly implemented utilizing `stan_gamm4` function from the rstanarm package and defined to be a logistic regression model with the help of the model parameter `family = binomial(link = 'logit')`.

The nonlinear relationship between the response variable `sigdz` and the explanatory variables is again defined with the help of the `formula` function. This time, passing the smoothing function `s()` for all continuous explanatory variables, to allow for more complexity in the model, while simultaneously penalizing over-fitting of the model with the the thin plate regression splines smoothness.

The same priors are used as for the linear model for both the intercept and the regression coefficients.

```{r}
#| label: Nonlinear model definition
#| warning: false
#| message: false

# Formula
formula_nonlinear <- formula(sigdz ~ sex + s(age) + s(cad_dur) + s(choleste))

# Model definition
model_nonlinear <- stan_gamm4(
  formula_nonlinear, data = cath,
  family = binomial(link = "logit"),
  # prior = prior_linear, prior_intercept = prior_linear,
  refresh = 0
)

# saveRDS(model_nonlinear, file = "./additional_files/model_nonlinear.rds")
# model_nonlinear <- readRDS("./additional_files/model_nonlinear.rds")
```

# Model Evaluation

After fitting the models, multiple evaluation metrics such as $\hat{R}$, effective sample size (ESS) and number of divergent transitions were used to assess the convergence of MCMC chains separately for each model. Additionally, we perform posterior predictive checks, assess the model performances as well as compare the models utilizing leave-one-out cross validation (LOO-CV). Finally, we perform prior sensitivity analysis for both models.

-   Rhat, ESS, HMC divergences, pp_check(), loo_compare(), classification accuracy
-   Prior sensitivity analysis

## Convergence diagnostics & posterior predictive checks

For the linear model, the posterior predictive check and posterior distributions of the parameters with 95 % credible interval are visualized in @linear-model-plots. AS we can see, the model fits the data relatively well, with a bit of a variation around the probability interval endpoints.

```{r}
#| label: linear-model-plots
#| fig-cap: TBD
#| echo: false

# posteriors
pplot_linear<-plot(model_linear, "areas", prob = 0.95, prob_outer = 1)
pplot_linear + geom_vline(xintercept = 0) +
labs(title="TBD")
# pp check
pp_check_linear <- brms::pp_check(model_linear) +
labs(title="TBD")
grid.arrange(pplot_linear, pp_check_linear, ncol=2) 
```

The convergence diagnostics for the linear model are summarized below.

```{r}
#| label: Linear-model-Convergence-diagnostics
#| echo: false

Rhat_linear <- model_linear$stan_summary[, "Rhat"] %>% round(3)
np_linear <- nuts_params(model_linear)
divergents_linear <- sum(subset(np_linear, Parameter == "divergent__")$Value)
ess_ratio_linear <- neff_ratio(model_linear)

cat("Linear model: \n")
cat("Rhat\n")
print(Rhat_linear)
cat("Number of divergent transitions: ", divergents_linear, "\n")
cat("ESS ratio: \n")
print(ess_ratio_linear)

```

The HMC chains have converged, as all $\hat{R}$ values are below 0.01. Additionally, there were no divergent transitions during convergence, and thus the simulation is reliable. The ratio of the ESS to the true sample size is over 1 with all explanatory variables as well as the intercept. This is

-   If ESS \> n -\> Posterior close to Gaussian/Gaussian like

For the nonlinear model...

-   With default prior number of divergent transitions is 1

-   With t-distr. prior we get 1 divergent transition.

```{r}
#| label: Nonlinear-model-Convergence-diagnostics
#| echo: true

# summary(model_nonlinear)

Rhat_nonlinear <- model_nonlinear$stan_summary[, "Rhat"] %>% round(3)
np_nonlinear <- nuts_params(model_nonlinear)
divergents_nonlinear <- sum(subset(np_nonlinear, Parameter == "divergent__")$Value)
ess_ratio_nonlinear <- neff_ratio(model_nonlinear)

cat("Nonlinear model: \n")
cat("Rhat\n")
print(head(Rhat_nonlinear))
cat("Number of divergent transitions: ", divergents_nonlinear, "\n")
cat("ESS ratio: \n")
print(head(ess_ratio_nonlinear))
```

## Model comparison using LOO-CV

```{r}
#| label: Computing LOOs
# LOO of our models
loo_linear <- loo(model_linear, save_psis = TRUE)
loo_nonlinear <- loo(model_nonlinear, save_psis = TRUE)
print(loo_linear)
print(loo_nonlinear)

# Baseline model of the linear model
model_baseline <- update(model_linear, formula = sigdz ~ 1, QR = FALSE)

# LOO of the baseline model
loo_baseline <- loo(model_baseline, save_psis = TRUE)
```

Comparing the LOOs:

```{r}
#| label: Comparing LOOs
loo_compare(loo_linear, loo_nonlinear, loo_baseline)
```

The nonlinear model seems to be the best!

## Posterior predictive performance: Classification accuracy

### Linear model:

```{r}
#| label: Linear model - Predictive probs
## Predicted probabilities
linpred_linear <- posterior_linpred(model_linear)
preds_linear <- posterior_epred(model_linear)
pred_linear <- colMeans(preds_linear)
pr_linear <- as.integer(pred_linear >= 0.5)
   
## posterior classification accuracy
# round(mean(xor(pr_linear,as.integer(y==0))),2)

# # posterior balanced classification accuracy
# round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Linear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_linear=E_loo(preds_linear, loo_linear$psis_object, type="mean", log_ratios = -log_lik(model_linear))$value

# LOO classification accuracy
round(mean(xor(ploo_linear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_linear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Nonlinear model

```{r}
#| label: Nonlinear model - Predictive probs
## Predicted probabilities
linpred_nonlinear <- posterior_linpred(model_nonlinear)
preds_nonlinear <- posterior_epred(model_nonlinear)
pred_nonlinear <- colMeans(preds_nonlinear)
pr_nonlinear <- as.integer(pred_nonlinear >= 0.5)
   
## posterior classification accuracy
# round(mean(xor(pr_nonlinear,as.integer(y==0))),2)

# # posterior balanced classification accuracy
# round((mean(xor(pr_linear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

LOO balanced (Better estimate, maybe let's include only these?):

```{r}
#| label: Nonlinear model - LOO balanced predictive probs
# LOO predictive probabilities
ploo_nonlinear=E_loo(preds_nonlinear, loo_nonlinear$psis_object, type="mean", log_ratios = -log_lik(model_nonlinear))$value

# LOO classification accuracy
round(mean(xor(ploo_nonlinear>0.5,as.integer(y==0))),2)

# LOO balanced classification accuracy
round((mean(xor(ploo_nonlinear[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo_nonlinear[y==1]<0.5,as.integer(y[y==1]))))/2,2)
```

### Calibration curves

```{r}
#| label: Calibration plots
#| fig-cap: TBD X
cplot_linear <- ggplot(data = data.frame(pred=pred_linear,loopred=ploo_linear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) +
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') +
  labs(x = "Predicted (LOO)",y = "Observed",title = "Linear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) +
  scale_y_continuous(breaks=seq(0,1,by=0.1)) +
  xlim(c(0,1)) +
  theme_minimal()

cplot_nonlinear <- ggplot(data = data.frame(loopred=ploo_nonlinear,
  y=as.numeric(y)-1), aes(x=loopred, y=y)) + 
  stat_smooth(method='glm', formula = y ~ ns(x, 5), fullrange=TRUE, color="deeppink") +
  geom_abline(linetype = 'dashed') + 
  labs(x = "Predicted (LOO)",y = "Observed",title = "Nonlinear model - Calibration plot") +
  geom_jitter(height=0.02, width=0, alpha=0.05) + 
  scale_y_continuous(breaks=seq(0,1,by=0.1)) + 
  xlim(c(0,1)) +
  theme_minimal() 

grid.arrange(cplot_linear, cplot_nonlinear, ncol=2)
```

## Prior sensitivity analysis

# Discussion

-   Problems, potential improvements?
-   Conclusion of analysis

# Lessons Learned

# References