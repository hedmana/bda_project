---
title: "BDA PROJECT"
author: Axel, Linnea, Elissa
format: 
  html:
    toc: true
    code-tools: true
    code-line-numbers: true  
    number-sections: true
    mainfont: Georgia, serif
    page-layout: article
  pdf:  
    geometry:
    - left=1cm,top=1cm,bottom=1cm,right=7cm
    number-sections: true
    code-annotations: none
editor: source
---

# Set up

## Loading the required packages

```{r}
library(ggplot2)
library(dplyr)
```

## Data

```{r}
library(survival)
data(transplant)
min(transplant$year)
max(transplant$year)
```

```{r}
ggplot(transplant, aes(x = futime)) + geom_histogram()
```

```{r}
transplant <- transplant %>%
                  filter(event != 'censored')

bins <- c(0, 100, 200, 300, 400, 500, Inf)

labels <- c('0-100', '101-200', '201-300', '301-400', '401-500', '>500')

transplant <- transplant %>%
  mutate(time_group = cut(futime, breaks = bins, labels = labels, include.lowest = TRUE, right = FALSE))
  
fractions <- transplant %>%
      group_by(year, abo) %>%
      summarise(sum(event == 'death'), .groups = 'drop')

colnames(fractions) <- c("days", "blood_type", "death")

fractions
```

```{r}
ggplot(fractions, aes(x = days, y = death, group = blood_type, color = blood_type)) + geom_line()
```

SCALE DATA??? SCALE BASED ON NUM BLOOD TYPE? 

# Model testing

## Model 1. Simple Bayesian regression on modelling events 'death' and 'ltx', i.e. receiving a liver transplant

```{r}
head(transplant)
transplant$id <- 1:nrow(transplant)
transplant_filtered <- transplant %>%
  filter(event %in% c("death", "ltx"))
```

On behalf of chatGPT:

```{r}
library(brms)

# Assuming your data frame is named 'transplant_data'
# 'Event' is your binary outcome variable (1 = event occurred, 0 = event did not occur)

formula <- bf(event ~ year + age + sex + abo + (1|id), family = bernoulli())

# Here, 'id' is a unique identifier for each person in your dataset.
# The random effect (1|id) accounts for the potential correlation between repeated measurements on the same person.

prior <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = b)
)

get_prior(
  formula,
  transplant_filtered,
  family = bernoulli()
)

# You can adjust the prior distributions based on your knowledge or assumptions about the data.

# Fit the model
model <- brm(formula, data = transplant_filtered, prior = prior, family = bernoulli(), chains = 4, iter = 2000)

```


```{r}
model
summary(model)
```

## 2. Model: Bayesian Regression with Multinomial Logistic Regression

```{r}
head(transplant)
transplant_filtered_2 <- transplant %>%
  filter(event != "censored")
```

```{r}
# Assuming your data frame is named 'transplant_data'
# 'Outcome' is your categorical outcome variable with 4 levels

formula_2 <- bf(event ~ year + age + sex + abo + (1|id), family = categorical())

prior_2 <- c(
  prior(normal(0, 10), class = Intercept, dpar = multx),
  prior(normal(0, 10), class = Intercept, dpar = muwithdraw),
  prior(normal(0, 10), class = b, dpar = multx),
  prior(normal(0, 10), class = b, dpar = muwithdraw)
)

get_prior(
  formula_2,
  transplant_filtered_2,
  family = categorical()
)
# You can adjust the prior distributions based on your knowledge or assumptions about the data.

# Fit the model
model_2 <- brm(formula_2, data = transplant_filtered_2, prior = prior_2, family = categorical(), chains = 4, iter = 2000)
```

```{r}
summary(model_2)
pp_check(model_2)

# ppc_bars
```

## Test: Estimating generalization error

Define test and train data:

```{r}
transplant <- transplant %>%
  filter(event != "censored")

n = nrow(transplant)

# Indexes
death_idx <- which(transplant$event == "death")
ltx_idx <- which(transplant$event == "ltx")
withdraw_idx <- which(transplant$event == "withdraw")

# Proportions
death_prop <- length(death_idx)/n
ltx_prop <- length(ltx_idx)/n
withdraw_prop <- length(withdraw_idx)/n

# Test and train data split
test_size = 100
death_idx_test <- sample(death_idx, size=round(test_size*death_prop), replace = FALSE)
ltx_idx_test <- sample(ltx_idx, size=round(test_size*ltx_prop), replace = FALSE)
withdraw_idx_test <- sample(withdraw_idx, size=round(test_size*withdraw_prop), replace = FALSE)

death_idx_tr <- setdiff(death_idx, death_idx_test)
ltx_idx_tr <- setdiff(ltx_idx, ltx_idx_test)
withdraw_idx_tr <- setdiff(withdraw_idx, withdraw_idx_test)

test_data <- transplant[c(death_idx_test, ltx_idx_test, withdraw_idx_test),]

train_data <- transplant[c(death_idx_tr, ltx_idx_tr, withdraw_idx_tr),]

# nrow(test_data)
# nrow(train_data)

# table(test_data$event)/nrow(test_data)
# table(train_data$event)/nrow(train_data)
```


Defining the model:

```{r}
formula_3 <- bf(event ~ year + age + sex + abo + (1|id), family = categorical())

prior_3 <- c(
  prior(normal(0, 10), class = Intercept, dpar = multx),
  prior(normal(0, 10), class = Intercept, dpar = muwithdraw),
  prior(normal(0, 10), class = b, dpar = multx),
  prior(normal(0, 10), class = b, dpar = muwithdraw)
)

# You can adjust the prior distributions based on your knowledge or assumptions about the data.

# Fit the model
model_3 <- brm(formula_3, data = train_data, prior = prior_3, family = categorical(), chains = 4, iter = 2000)
```

Check:

```{r}
summary(model_3)
pp_check(model_3)
```

Predictions:

```{r}
preds <- posterior_predict(model_3, newdata = select(test_data, -event), allow_new_levels=TRUE)

preds

test_data <- test_data %>%
  mutate(event_int = ifelse(
    event == "death", 1, ifelse(
      event == "ltx", 2, 3
  )))

preds_mean <- colMeans(preds) %>% round()
sum(preds_mean == test_data$event_int)
preds_mean
test_data$event_int
```

# Test: 2nd take

To handle imbalanced data:

```{r}
install.packages("smotefamily")
library(smotefamily)
```